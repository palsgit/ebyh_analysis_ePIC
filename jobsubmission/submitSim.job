# submitReco.job -- submit reco

Universe        		= vanilla
GetEnv          		= False
#InteractiveJob 	    = False

# The requirement line specifies which machines we want to run this job on.  Any arbitrary classad expression can be used.
Requirements    = (CPU_Speed >= 1)

# Rank is an expression that states how to rank machines which have already met the requirements expression. Essentially, 
# rank expresses preference.  A higher numeric value equals better rank.  Condor will give the job the machine with the highest rank.
Rank		= CPU_Speed

# Jobs by default get 1.4Gb of RAM allocated, ask for more if needed but if a job needs
#more than 2Gb it will not be able to run on the older nodes
#request_memory = 1800M

# If you need multiple cores you can ask for them, but the scheduling may take longer the "larger" a job you ask for
#request_cpus = 1

# Used to give jobs a directory with respect to file input and output.

Initialdir              = /gpfs/mnt/gpfs02/eic/palspeic/EpicHcalAnalysis/ebyh/
Outputdir               = /gpfs/mnt/gpfs02/eic/palspeic/simdir/ebyh_response/Steel/

executable              = executable.sh

#when_to_transfer_output = ON_EXIT
#transfer_input_files    = fithistos_1D.C, readHCalRecoReader.C
#should_transfer_files   = YES
#transfer_output_files   = ./


Error                  = $(Outputdir)/log/$(Cluster)_$(Process).err
Output                  = $(Outputdir)/log/$(Cluster)_$(Process).out
Log                     = $(Outputdir)/log/$(Cluster)_$(Process).log

Queue 100

